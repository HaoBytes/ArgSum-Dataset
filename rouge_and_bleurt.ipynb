{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sROeQuuFrZXJ",
        "outputId": "7a650d59-0c36-4f32-feff-7536316e741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=7cbd255cdbe995cd6695957eb37b7303ad10b29fef37037c677893966628117a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import inflect\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "from itertools import product\n",
        "from statistics import mean\n",
        "import json\n",
        "import inflect\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "f0THhwCE0Fqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise_rouge(df, column_name_1, column_name_2):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL',\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "    scores = []\n",
        "    for i in range(len(df)):\n",
        "        score = scorer.score(df.iloc[i][column_name_1], df.iloc[i][column_name_2])\n",
        "\n",
        "        scores.append({\n",
        "            \"document\": i,\n",
        "            \"rouge_1\": score[\"rouge1\"].fmeasure,\n",
        "            \"rouge_2\": score[\"rouge2\"].fmeasure,\n",
        "            \"rouge_L\": score[\"rougeL\"].fmeasure,\n",
        "            \"rouge_Lsum\": score[\"rougeLsum\"].fmeasure\n",
        "        })\n",
        "\n",
        "    return scores\n",
        "\n",
        "def compute_rouge_for_models(dataframe):\n",
        "    reference_models = ['gpt4']\n",
        "    candidate_models = dataframe['model'].unique()\n",
        "    reference_settings = ['top2_golden']\n",
        "    candidate_settings = ['top2_golden']\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
        "\n",
        "    rouge_scores = []\n",
        "\n",
        "    for model in candidate_models:\n",
        "        if model not in reference_models:\n",
        "            for setting in candidate_settings:\n",
        "                # Filter rows for the candidate model and setting\n",
        "                filtered_df = dataframe[(dataframe['model'] == model) & (dataframe['setting'] == setting)]\n",
        "\n",
        "                # Compute ROUGE scores against the reference models and settings\n",
        "                for ref_model in reference_models:\n",
        "                    for ref_setting in reference_settings:\n",
        "                        reference_text = dataframe[(dataframe['model'] == ref_model) & (dataframe['setting'] == ref_setting)]['concat_column'].values[0]\n",
        "                        candidate_text = filtered_df['concat_column'].values[0]\n",
        "                        scores = scorer.score(reference_text, candidate_text)\n",
        "\n",
        "                        rouge_scores.append({\n",
        "                            \"candidate_model\": model,\n",
        "                            \"candidate_setting\": setting,\n",
        "                            \"reference_model\": ref_model,\n",
        "                            \"reference_setting\": ref_setting,\n",
        "                            \"rouge_1\": scores[\"rouge1\"].fmeasure,\n",
        "                            \"rouge_2\": scores[\"rouge2\"].fmeasure,\n",
        "                            \"rouge_L\": scores[\"rougeL\"].fmeasure,\n",
        "                            \"rouge_Lsum\": scores[\"rougeLsum\"].fmeasure\n",
        "                        })\n",
        "\n",
        "    return rouge_scores"
      ],
      "metadata": {
        "id": "bi4p5HEE0UIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb5oGRFc_kcE"
      },
      "outputs": [],
      "source": [
        "csv_files = [\"task3_summary_trainset.csv\", \"task3_summary_devset.csv\", \"task3_summary_testset.csv\"]\n",
        "dfs = [pd.read_csv(file) for file in csv_files]\n",
        "concatenated_df = pd.concat(dfs)\n",
        "concatenated_df = concatenated_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_rouge_for_models(concatenated_df)"
      ],
      "metadata": {
        "id": "cIOKY9Qw2Udd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}